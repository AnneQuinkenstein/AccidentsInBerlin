{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T18:43:53.850580Z",
     "start_time": "2024-07-27T18:43:53.134201Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, fbeta_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T18:43:53.893843Z",
     "start_time": "2024-07-27T18:43:53.851346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# daten laden / teilen\n",
    "df = pd.read_csv('../data/GeneralDatensatz18-21ohneGeo-mitLockdown_mitCorona_mitF.csv', sep=';')\n",
    "\n",
    "X=df[['UMONAT','USTUNDE','UWOCHENTAG','UART','USTRZUSTAND','BEZ','UTYP1','ULICHTVERH','IstRad','IstPKW','IstFuss','IstKrad','IstGkfz','IstSonstige', 'LOCKDOWN', 'COVID']]\n",
    "#für tödliche Unfälle\n",
    "\n",
    "#für tödliche und schwere vs. leichte Unfälle  -> 1 ist schwer oder tödlich, 0 ist leicht\n",
    "y = df['UKATEGORIE'].isin([1, 2]).astype(int)\n"
   ],
   "id": "3c17ed6e09fa6cec",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T18:43:53.896904Z",
     "start_time": "2024-07-27T18:43:53.894518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# StratifiedKFold initialisieren\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Variablen für Ergebnisse\n",
    "fbetas_rf = []\n",
    "fbetas_random = []\n",
    "fbetas_mehrheit = []"
   ],
   "id": "14640e88b6468603",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T18:43:53.899799Z",
     "start_time": "2024-07-27T18:43:53.897993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "beta = 2\n",
    "fbeta_scorer= make_scorer(fbeta_score, beta=beta)"
   ],
   "id": "fcd1f4cf70853c46",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T18:43:54.426876Z",
     "start_time": "2024-07-27T18:43:53.900688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stratified K-Fold Cross-Validation\n",
    "counter = 0\n",
    "for train_index, test_index in skf.split(df.iloc[:, :-1], df['UKATEGORIE'].isin([1, 2]).astype(int)):\n",
    "    X_train, X_test = df.iloc[train_index, :-1], df.iloc[test_index, :-1]\n",
    "    y_train, y_test = df.iloc[train_index, -1], df.iloc[test_index, -1]\n",
    "    \n",
    "    \n",
    "    # Random Forest Modell trainieren\n",
    "    model = LogisticRegression(max_iter=150, C=0.069,  solver='lbfgs', penalty='l2', tol=0.001, class_weight= {0: 1, 1: 9})\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Vorhersagen mit dem Random Forest Modell\n",
    "    y_pred_rf = model.predict(X_test)\n",
    "    \n",
    "    # Leistung des Random Forest Modells bewerten\n",
    "    fbeta_rf = fbeta_score(y_test, y_pred_rf, beta=beta, average='weighted')\n",
    "    print(f\"Fold nummer {counter}: score ist {fbeta_rf} \")\n",
    "    fbetas_rf.append(fbeta_rf)\n",
    "    \n",
    "     # jetzt mit random:\n",
    "\n",
    "    # Klassenverteilung im Testdatensatz ermitteln\n",
    "    class_counts = Counter(y_test)\n",
    "    total_samples = len(y_test)\n",
    "    class_probabilities = {cls: count / total_samples for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Zufällige Vorhersagen basierend auf den Klassenwahrscheinlichkeiten erstellen\n",
    "    np.random.seed(42)\n",
    "    y_pred_random_weighted = np.random.choice(\n",
    "        list(class_probabilities.keys()),\n",
    "        size=y_test.shape,\n",
    "        p=list(class_probabilities.values())\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Leistung der gewichteten zufälligen Vorhersagen bewerten\n",
    "    fbeta_random_weighted = fbeta_score(y_test, y_pred_random_weighted, beta=beta, average='weighted')\n",
    "   \n",
    "    fbetas_random.append(fbeta_random_weighted)\n",
    "    \n",
    "    # immer mehrheitsklasse vorhersagen\n",
    "    y_pred_zeros =np.zeros(y_test.shape)\n",
    "   \n",
    "    # Leistung der Vorhersagen der mehrheitsklasse bewerten\n",
    "    fbeta_mehrheit = fbeta_score(y_test, y_pred_zeros, beta=beta, average='weighted')\n",
    "    fbetas_mehrheit.append(fbeta_mehrheit)\n",
    "    counter = counter+1\n",
    "    \n",
    "    "
   ],
   "id": "f6ccdf6e33312554",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Random Forest Modell trainieren\u001B[39;00m\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m LogisticRegression(max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m, C\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.069\u001B[39m,  solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlbfgs\u001B[39m\u001B[38;5;124m'\u001B[39m, penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml2\u001B[39m\u001B[38;5;124m'\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, class_weight\u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m: \u001B[38;5;241m9\u001B[39m})\n\u001B[0;32m---> 11\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Vorhersagen mit dem Random Forest Modell\u001B[39;00m\n\u001B[1;32m     14\u001B[0m y_pred_rf \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1196\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1193\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1194\u001B[0m     _dtype \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mfloat64, np\u001B[38;5;241m.\u001B[39mfloat32]\n\u001B[0;32m-> 1196\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[1;32m   1197\u001B[0m     X,\n\u001B[1;32m   1198\u001B[0m     y,\n\u001B[1;32m   1199\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1200\u001B[0m     dtype\u001B[38;5;241m=\u001B[39m_dtype,\n\u001B[1;32m   1201\u001B[0m     order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1202\u001B[0m     accept_large_sparse\u001B[38;5;241m=\u001B[39msolver \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaga\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1203\u001B[0m )\n\u001B[1;32m   1204\u001B[0m check_classification_targets(y)\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    582\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 584\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[1;32m    585\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    587\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1106\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1101\u001B[0m         estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[1;32m   1102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1104\u001B[0m     )\n\u001B[0;32m-> 1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[1;32m   1107\u001B[0m     X,\n\u001B[1;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   1109\u001B[0m     accept_large_sparse\u001B[38;5;241m=\u001B[39maccept_large_sparse,\n\u001B[1;32m   1110\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m   1111\u001B[0m     order\u001B[38;5;241m=\u001B[39morder,\n\u001B[1;32m   1112\u001B[0m     copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[1;32m   1113\u001B[0m     force_all_finite\u001B[38;5;241m=\u001B[39mforce_all_finite,\n\u001B[1;32m   1114\u001B[0m     ensure_2d\u001B[38;5;241m=\u001B[39mensure_2d,\n\u001B[1;32m   1115\u001B[0m     allow_nd\u001B[38;5;241m=\u001B[39mallow_nd,\n\u001B[1;32m   1116\u001B[0m     ensure_min_samples\u001B[38;5;241m=\u001B[39mensure_min_samples,\n\u001B[1;32m   1117\u001B[0m     ensure_min_features\u001B[38;5;241m=\u001B[39mensure_min_features,\n\u001B[1;32m   1118\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[1;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1120\u001B[0m )\n\u001B[1;32m   1122\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m   1124\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    915\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    916\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    917\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m    918\u001B[0m         )\n\u001B[1;32m    920\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m--> 921\u001B[0m         _assert_all_finite(\n\u001B[1;32m    922\u001B[0m             array,\n\u001B[1;32m    923\u001B[0m             input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[1;32m    924\u001B[0m             estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[1;32m    925\u001B[0m             allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    926\u001B[0m         )\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    929\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    160\u001B[0m     )\n\u001B[0;32m--> 161\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc0c6383091941af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Durchschnittliche Genauigkeiten berechnen\n",
    "mean_scores_rf = np.mean(fbetas_rf)\n",
    "mean_scores_random = np.mean(fbetas_random)\n",
    "mean_scores_mehrheit = np.mean(fbetas_mehrheit)"
   ],
   "id": "dcd08cc94652934f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Random Forest Mean score: {mean_scores_rf}\")\n",
    "print(f\"Weighted Random Prediction Mean score: {mean_scores_random}\")\n",
    "print(f\"Weighted Mehrheit Prediction Mean score: {mean_scores_mehrheit}\")\n"
   ],
   "id": "76d1e18435ebe2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Klassifikationsberichte für den letzten Fold ausgeben\n",
    "print(\"Random Forest Classification Report (Last Fold):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Weighted Random Prediction Classification Report (Last Fold):\")\n",
    "print(classification_report(y_test, y_pred_random_weighted))"
   ],
   "id": "67d40de0fe392e46",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
